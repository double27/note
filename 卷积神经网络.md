
# 一. 卷积神经网络介绍
卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络，是深度学习的代表算法之一。
1998年LeNet-5在手写数字的识别问题中取得的成功使卷积神经网络的应用得到关注，微软在2003年使用卷积神经网络开发了光学字符读取（Optical Character Recognition, OCR）系统。
但是这个模型在后来的一段时间并未能火起来，主要原因是费机器（当时没有GPU），而且其他的算法（SVM等）也能达到类似的效果甚至超过。
随着ReLU和dropout的提出，以及GPU和大数据带来的历史机遇，CNN在2012年迎来了历史突破–AlexNet.
2012年，Hinton的学生Alex Krizhevsky在寝室用GPU死磕了一个Deep Learning模型，一举摘下了视觉领域竞赛ILSVRC 2012的桂冠，在百万量级的ImageNet数据集合上，效果大幅度超过传统的方法，从传统的70%多提升到80%多。
这个Deep Learning模型就是后来大名鼎鼎的AlexNet模型。AlexNet的成功主要由以下三个方面的原因：
1. 大量数据，李飞飞团队开源了大的标注数据集合ImageNet；
2. GPU，这种高度并行的计算神器确实助了洪荒之力，没有神器在手，Alex估计不敢搞太复杂的模型；
3. 算法的改进，包括网络变深、数据增强、ReLU、Dropout等。

自此，Deep Learning一发不可收拾，ILSVRC每年都不断被Deep Learning刷榜。
以下是ILSVRC竞赛2012年开始冠军队伍用的几大模型：
![GitHub](./picture/ILSVRC.png "ILSVRC竞赛冠军模型")

卷积神经网络与普通[神经网络](http://deeplearning.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C)的区别在于，卷积神经网络包含了一个由卷积层和子采样层构成的特征抽取器。在卷积神经网络的卷积层中，一个神经元只与部分邻层神经元连接。在CNN的一个卷积层中，通常包含若干个特征平面(featureMap)，每个特征平面由一些矩形排列的的神经元组成，同一特征平面的神经元共享权值，这里共享的权值就是卷积核。卷积核一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值（卷积核）带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化（pooling），通常有均值子采样（mean pooling）和最大值子采样（max pooling）两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。卷积神经网络的基本结构如图所示：


# 二. 卷积神经网络的层级结构
![GitHub](./picture/cnn_model.png "卷积神经网络的基本结构")
卷积神经网络的层级结构：
- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层 / ReLU layer
- 池化层 / Pooling layer
- 全连接层 / FC layer
- 输出层

## 1. 数据输入层
数据输入层主要对原始图像做处理，其中包括：
- 去均值
- 归一化
- PCA/白化
### 1. 去均值
把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。
### 2. 归一化
幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。

去均值与归一化效果图：
![去均值与归一化效果图](./picture/normalized.jpg "去均值归一化")
### 3. PCA/白化
用PCA降维；白化是对数据各个特征轴上的幅度归一化

去相关与白化效果图：
![去相关与白化效果图](./picture/whitened.jpg "去相关与白化效果图")
## 2. 隐含层
卷积神经网络的隐含层包含卷积层、池化层和全连接层3类常见构筑，在一些更为现代的算法中可能有Inception模块、残差块（residual block）等复杂构筑。在常见构筑中，卷积层和池化层为卷积神经网络特有。卷积层中的卷积核包含权重系数，而池化层不包含权重系数，因此在文献中，池化层可能不被认为是独立的层。以LeNet-5为例，3类常见构筑在隐含层中的顺序通常为：输入-卷积层-池化层-卷积层-池化层-全连接层-输出。
### 1. 卷积层
1. 卷积
    >图像中不同数据窗口的数据和卷积核（一个滤波矩阵）作内积的操作叫做卷积。其计算过程又称为滤波（filter)，本质是提取图像不同频段的特征。
2. 卷积核
    >卷积层的功能是对输入数据进行特征提取，其内部包含多个卷积核，组成卷积核的每个元素都对应一个权重系数和一个偏差量（bias vector），类似于一个前馈神经网络的神经元（neuron）。卷积层内每个神经元都与前一层中位置接近的区域的多个神经元相连，区域的大小取决于卷积核的大小，在文献中被称为“感受野（receptive field）”。<br>
    <br>一个卷积核的滤波可以用来提取特定的特征（例如可以提取物体轮廓、颜色深浅等）。通过卷积层从原始数据中提取出新的特征的过程又成为feature map(特征图)。filter_size是指filter的大小，例如3\*3； 
    <br><br>filter_num是指每种filter_size的filter个数，通常是通道个数
3. 卷积层
    >多个滤波器叠加便成了卷积层
4. 卷积层的参数
    >卷积层参数包括卷积核大小、步长和填充，三者共同决定了卷积层输出特征图的尺寸，是卷积神经网络的超参数。
    <br><br>其中卷积核大小可以指定为小于输入图像尺寸的任意值，卷积核越大，可提取的输入特征越复杂。
    <br><br>卷积步长定义了卷积核相邻两次扫过特征图时位置的距离，卷积步长为1时，卷积核会逐个扫过特征图的元素，步长为n时会在下一次扫描跳过n-1个像素。
    <br><br>填充是在特征图通过卷积核之前人为增大其尺寸以抵消计算中尺寸收缩影响的方法。常见的填充方法为按0填充和重复边界值填充
5. 通道
    >通道可以理解为视角、角度。例如同样是提取边界特征的卷积核，可以按照R、G、B三种元素的角度提取边界，RGB在边界这个角度上有不同的表达；再比如需要检查一个人的机器学习能力，可以从特征工程、模型选择、参数调优等多个方面检测

卷积层的主要特征:
- 局部感知

    - 在传统的神经网络中，每个神经元都与图片上的每一个像素相连接，这样会造成权重参数数量巨大而无法训练。<br>
    - 一般认为图像的空间联系是局部的像素联系比较密切，而距离较远的像素相关性较弱，因此，每个神经元没必要对全局图像进行感知，只要对局部进行感知，然后在更高层将局部的信息综合起来得到全局信息。<br>
    - 在卷积神经网络中，每个神经元的权重个数都是卷积核的大小，这就相当于每个神经元只与部分像素相连接,这样就极大的减少了权重的数量。
![局部感知](./picture/jubuganzhi.png "局部感知直观图")    
    
- 权值共享
    
    权值共享实际上就是局部感知的部分，用一个卷积核从一个局部区域学习到的信息，应用到图像的其它地方去。即用该卷积核去卷积整张图，生成的feature map的每一个像素值都是由这个卷积核产生的，这就是权值共享。

![权值共享](./picture/share_param.gif "权值共享") 
    

- 多核卷积

    一个卷积核操作只能得到一部分特征可能获取不到全部特征，这么一来我们就引入了多核卷积。用每个卷积核来学习不同的特征（每个卷积核学习到不同的权重）来提取原图特征。

下面的动态图形象地展示了卷积层的计算过程：
![卷积层的计算过程](./picture/convolution.gif "卷积层的计算过程")    


### 2. 激励函数
>神经网络中的每个神经元节点接受上一层神经元的输出值作为本神经元的输入值，并将输入值传递给下一层，输入层神经元节点会将输入属性值直接传递给下一层（隐层或输出层）。在多层神经网络中，上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。
<br><br>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。

![激励函数](./picture/relu.jpg "激励函数")
参考：
- [激活函数](https://blog.csdn.net/u014088052/article/details/50923924)
- [常用激活函数（激励函数）理解与总结](https://blog.csdn.net/tyhj_sf/article/details/79932893)
    
### 3. 池化层
>池化层主要用在连续的卷积层与激励层中间，用于压缩数据和参数的量，并减少过拟合，同时又保留有用信息。
<br>池化层的策略主要有max pooling和average pooling。下图展示的是max pooling的一个结果。
![最大池化](./picture/max_pooling.png "最大池化")

### 4. [全连接层](https://zhuanlan.zhihu.com/p/33841176)
>全连接层之前的作用是提取特征,全理解层的作用是分类
<br>全连接层把特征representation整合到一起，输出为一个值,就大大减少特征位置对分类带来的影响
<br>在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化

参考：
- [CNN 入门讲解：什么是全连接层](https://zhuanlan.zhihu.com/p/33841176)

## 二. 卷积神经网络的训练
卷积神经网络的训练主要由前向传播和反向传播。
- 前向传播: 输入的样本从输入层经过隐单元一层一层进行处理，通过所有的隐层之后，传向输出层。
- 反向传播: 把误差信号按原来正向传播的通路反向传回，并对每个隐层的各个神经元的权系数进行修改，以使误差信号趋向最小。


在反向创博中，主要应用到复合函数求导，梯度下降法
<br>训练算法有两个版本：批量模式和单样本模式。
- 批量模式每次梯度下降法迭代时对所有样本计算损失函数值，计算出对这些样本的总误差，然后用梯度下降法更新参数；
- 单样本模式是每次对一个样本进行前向传播，计算对该样本的误差，然后更新参数，它可以天然的支持增量学习，即动态的加入新的训练样本进行训练。

卷积神经网络中卷积层的权重更新过程本质是卷积核的更新过程。

参考：
- [反向传播算法推导-全连接神经网络](https://zhuanlan.zhihu.com/p/39195266)
- [CNN 入门讲解：图片在卷积神经网络中是怎么变化的（前向传播 Forward Propagation）](https://zhuanlan.zhihu.com/p/34222451)
## 三.卷积神经网络演变史



参考：
- [卷积神经网络-百度百科](https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/17541100?fr=aladdin)
- [卷积神经网络概念与原理](https://blog.csdn.net/yunpiao123456/article/details/52437794)
- [CNN网络架构演进：从LeNet到DenseNet](https://www.cnblogs.com/skyfsm/p/8451834.html)
- [CNN网络架构变迁](https://www.jianshu.com/p/1d76db232e4f)
- [CNN模型发展史](https://blog.csdn.net/weixin_41770169/article/details/84985254)
- [从AlexNet理解卷积神经网络的一般结构](https://blog.csdn.net/qq_30868235/article/details/80465722)
- [卷积神经网络CNN总结](https://www.cnblogs.com/skyfsm/p/6790245.html)
- [什么是全连接层](https://zhuanlan.zhihu.com/p/33841176)
- [CNN详解（卷积层及下采样层）](https://blog.csdn.net/baidu_14831657/article/details/60570765)


